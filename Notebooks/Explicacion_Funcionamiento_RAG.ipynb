{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExplicaciÃ³n del Funcionamiento del Sistema RAG\n",
    "\n",
    "Este notebook explica de manera simple y paso a paso cÃ³mo funciona el sistema completo de GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) para consultar discursos del Presidente Javier Milei. Cubriremos desde la ingesta inicial de datos hasta la consulta final en la interfaz.\n",
    "\n",
    "**Requisitos previos:**\n",
    "- Archivo `mi_corpus.json` con los discursos procesados\n",
    "- Variable de entorno `GOOGLE_API_KEY` configurada con tu clave de API de Google\n",
    "- Bibliotecas instaladas (ver requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar el directorio padre al path para importar mÃ³dulos locales\n",
    "sys.path.append('..')\n",
    "\n",
    "# Importar funciones del sistema RAG\n",
    "from mvp_rag import load_discursos_from_file, crear_vectorstore, configurar_rag, consultar_rag\n",
    "\n",
    "# Verificar que la API key estÃ© configurada\n",
    "if not os.getenv('GOOGLE_API_KEY'):\n",
    "    print(\"âš ï¸  Advertencia: GOOGLE_API_KEY no estÃ¡ configurada. ConfigÃºrala antes de continuar.\")\n",
    "else:\n",
    "    print(\"âœ… GOOGLE_API_KEY configurada correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GeneraciÃ³n del Corpus\n",
    "\n",
    "El primer paso en el sistema RAG es crear un corpus de documentos. Esto implica:\n",
    "\n",
    "1. **Scraping de datos:** Recopilar discursos desde fuentes externas (como el sitio web de Casa Rosada)\n",
    "2. **Procesamiento de textos:** Limpiar y estructurar el contenido de los discursos\n",
    "3. **Almacenamiento:** Guardar los discursos en un archivo JSON para uso posterior\n",
    "\n",
    "En este proyecto, el corpus ya estÃ¡ preprocesado y almacenado en `mi_corpus.json`. Cada documento contiene:\n",
    "- `titulo`: TÃ­tulo del discurso\n",
    "- `contenido_limpio`: Texto procesado del discurso\n",
    "- `fecha_publicacion`: Fecha del discurso\n",
    "- `url`: Enlace al discurso original\n",
    "\n",
    "**Nota:** Para generar un nuevo corpus desde cero, se ejecutarÃ­a `generate_corpus.py`, que utiliza scraping web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el corpus de discursos desde el archivo\n",
    "print(\"Cargando discursos desde mi_corpus.json...\")\n",
    "discursos = load_discursos_from_file()\n",
    "print(f\"âœ… Se cargaron {len(discursos)} discursos exitosamente.\")\n",
    "\n",
    "# Mostrar informaciÃ³n bÃ¡sica del corpus\n",
    "print(f\"\\nðŸ“Š EstadÃ­sticas del corpus:\")\n",
    "print(f\"- NÃºmero total de discursos: {len(discursos)}\")\n",
    "\n",
    "# Mostrar un ejemplo de discurso\n",
    "if discursos:\n",
    "    ejemplo = discursos[0]\n",
    "    print(f\"\\nðŸ“„ Ejemplo de discurso:\")\n",
    "    print(f\"TÃ­tulo: {ejemplo['titulo']}\")\n",
    "    print(f\"Fecha: {ejemplo['fecha_publicacion']}\")\n",
    "    print(f\"Contenido (primeros 200 caracteres): {ejemplo['contenido_limpio'][:200]}...\")\n",
    "    print(f\"URL: {ejemplo['url']}\")\n",
    "else:\n",
    "    print(\"âŒ No se encontraron discursos en el archivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Procesamiento y CreaciÃ³n del Vectorstore\n",
    "\n",
    "Una vez que tenemos el corpus, necesitamos procesarlo para crear un vectorstore que permita bÃºsquedas eficientes:\n",
    "\n",
    "1. **DivisiÃ³n de documentos:** Los discursos largos se dividen en fragmentos mÃ¡s pequeÃ±os (chunks) para mejor recuperaciÃ³n\n",
    "2. **Embeddings:** Cada fragmento se convierte en un vector numÃ©rico usando un modelo de embeddings\n",
    "3. **Vectorstore:** Los vectores se almacenan en una base de datos vectorial (ChromaDB) para bÃºsquedas por similitud\n",
    "\n",
    "El sistema utiliza diferentes estrategias de divisiÃ³n segÃºn la longitud del documento:\n",
    "- Documentos largos (>5000 caracteres): Chunks de 1000 caracteres con 200 de solapamiento\n",
    "- Documentos cortos (<1000 caracteres): Chunks de 300 caracteres con 50 de solapamiento\n",
    "- Documentos intermedios: Chunks de 500 caracteres con 50 de solapamiento\n",
    "\n",
    "Se utiliza el modelo de embeddings `intfloat/multilingual-e5-large` para manejar texto en espaÃ±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el vectorstore a partir de los discursos cargados\n",
    "print(\"Creando vectorstore... Esto puede tomar unos minutos.\")\n",
    "vectorstore = crear_vectorstore(discursos)\n",
    "print(\"âœ… Vectorstore creado y persistido exitosamente.\")\n",
    "\n",
    "# Verificar el vectorstore\n",
    "print(f\"\\nðŸ“Š InformaciÃ³n del vectorstore:\")\n",
    "print(f\"- Nombre de colecciÃ³n: documentos_discursos\")\n",
    "print(f\"- Directorio de persistencia: ./chroma_db\")\n",
    "\n",
    "# Probar una bÃºsqueda simple en el vectorstore\n",
    "print(\"\\nðŸ” Probando bÃºsqueda en vectorstore...\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "resultados_prueba = retriever.get_relevant_documents(\"economÃ­a\")\n",
    "print(f\"Se encontraron {len(resultados_prueba)} documentos relevantes para 'economÃ­a':\")\n",
    "for i, doc in enumerate(resultados_prueba, 1):\n",
    "    print(f\"{i}. {doc.metadata['titulo']} ({doc.metadata['fecha_publicacion']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConfiguraciÃ³n del Sistema RAG\n",
    "\n",
    "Con el vectorstore listo, configuramos el sistema RAG completo:\n",
    "\n",
    "1. **Modelo de Lenguaje (LLM):** Se utiliza Google Gemini 2.5-flash como modelo generativo\n",
    "2. **Prompt Template:** Plantilla especializada para consultas sobre discursos de Milei\n",
    "3. **Retriever:** Configurado con MMR (Maximal Marginal Relevance) para diversidad en resultados\n",
    "4. **Cadena RAG:** CombinaciÃ³n de recuperaciÃ³n y generaciÃ³n usando RetrievalQA\n",
    "\n",
    "El prompt estÃ¡ diseÃ±ado especÃ­ficamente para:\n",
    "- Proporcionar informaciÃ³n precisa basada Ãºnicamente en discursos oficiales\n",
    "- Estructurar respuestas con resumen ejecutivo, detalles especÃ­ficos y fuentes\n",
    "- Mantener un tono profesional y amigable\n",
    "- Indicar claramente cuando informaciÃ³n no estÃ¡ disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el sistema RAG\n",
    "print(\"Configurando sistema RAG...\")\n",
    "sistema_rag = configurar_rag(vectorstore)\n",
    "print(\"âœ… Sistema RAG configurado exitosamente.\")\n",
    "\n",
    "# Mostrar configuraciÃ³n del retriever\n",
    "print(f\"\\nâš™ï¸  ConfiguraciÃ³n del retriever:\")\n",
    "print(f\"- Tipo de bÃºsqueda: MMR (Maximal Marginal Relevance)\")\n",
    "print(f\"- NÃºmero de documentos a recuperar: 5\")\n",
    "print(f\"- Documentos candidatos iniciales: 15\")\n",
    "print(f\"- ParÃ¡metro lambda: 0.6\")\n",
    "\n",
    "# Mostrar configuraciÃ³n del LLM\n",
    "print(f\"\\nðŸ¤– ConfiguraciÃ³n del LLM:\")\n",
    "print(f\"- Modelo: gemini-2.5-flash\")\n",
    "print(f\"- Temperatura: 0.1\")\n",
    "print(f\"- Proveedor: Google Generative AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consulta en el Sistema RAG\n",
    "\n",
    "Finalmente, podemos realizar consultas al sistema RAG. El proceso funciona asÃ­:\n",
    "\n",
    "1. **Consulta del usuario:** Se recibe una pregunta en lenguaje natural\n",
    "2. **RecuperaciÃ³n:** El retriever busca los documentos mÃ¡s relevantes en el vectorstore\n",
    "3. **GeneraciÃ³n:** El LLM genera una respuesta basada en los documentos recuperados\n",
    "4. **Respuesta estructurada:** Se devuelve una respuesta con resumen, detalles y fuentes\n",
    "\n",
    "La interfaz web (app.py) permite consultas interactivas, pero aquÃ­ realizaremos una consulta de ejemplo directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta de ejemplo al sistema RAG\n",
    "pregunta_ejemplo = \"Â¿QuÃ© dice Milei sobre la economÃ­a argentina?\"\n",
    "\n",
    "print(f\"ðŸ’¬ Realizando consulta: '{pregunta_ejemplo}'\")\n",
    "print(\"Procesando...\")\n",
    "\n",
    "# Ejecutar la consulta\n",
    "resultado = consultar_rag(sistema_rag, pregunta_ejemplo)\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(\"\\nâœ… Respuesta del sistema RAG:\")\n",
    "print(\"=\" * 50)\n",
    "print(resultado['result'])\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mostrar informaciÃ³n sobre las fuentes\n",
    "fuentes = resultado.get('source_documents', [])\n",
    "print(f\"\\nðŸ“š Fuentes consultadas ({len(fuentes)} documentos):\")\n",
    "for i, fuente in enumerate(fuentes, 1):\n",
    "    metadata = fuente.metadata\n",
    "    print(f\"{i}. {metadata.get('titulo', 'Sin tÃ­tulo')} - {metadata.get('fecha_publicacion', 'Sin fecha')}\")\n",
    "    print(f\"   URL: {metadata.get('url', 'Sin URL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConclusiÃ³n\n",
    "\n",
    "Hemos visto cÃ³mo funciona el sistema RAG completo paso a paso:\n",
    "\n",
    "1. **GeneraciÃ³n del corpus:** RecopilaciÃ³n y procesamiento de discursos\n",
    "2. **Vectorstore:** CreaciÃ³n de representaciones vectoriales para bÃºsqueda eficiente\n",
    "3. **ConfiguraciÃ³n RAG:** IntegraciÃ³n de recuperaciÃ³n y generaciÃ³n\n",
    "4. **Consulta:** Respuestas precisas basadas en fuentes verificables\n",
    "\n",
    "Este sistema permite consultas inteligentes sobre discursos oficiales, proporcionando respuestas contextualizadas y con referencias a las fuentes originales.\n",
    "\n",
    "**Para usar en producciÃ³n:** Ejecuta `streamlit run app.py` para acceder a la interfaz web interactiva."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}